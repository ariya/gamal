#!/usr/bin/env node

const fs = require('fs');
const readline = require('readline');

const LLM_API_KEY = process.env.LLM_API_KEY;
const LLM_API_BASE_URL = process.env.LLM_API_BASE_URL || 'https://openrouter.ai/api/v1';
const LLM_CHAT_MODEL = process.env.LLM_CHAT_MODEL || 'openchat/openchat-8b';
const LLM_STREAMING = process.env.LLM_STREAMING !== 'no';

const TAVILY_API_KEY = process.env.TAVILY_API_KEY;
const TOP_K = 3;

const LLM_DEBUG_CHAT = process.env.LLM_DEBUG_CHAT;
const LLM_DEBUG_PIPELINE = process.env.LLM_DEBUG_PIPELINE;
const LLM_DEBUG_SEARCH = process.env.LLM_DEBUG_SEARCH;
const LLM_DEBUG_FAIL_EXIT = process.env.LLM_DEBUG_FAIL_EXIT;

const NORMAL = '\x1b[0m';
const BOLD = '\x1b[1m';
const YELLOW = '\x1b[93m';
const MAGENTA = '\x1b[35m';
const RED = '\x1b[91m';
const GREEN = '\x1b[92m';
const CYAN = '\x1b[36m';
const GRAY = '\x1b[90m';
const ARROW = '⇢';
const CHECK = '✓';
const CROSS = '✘';

/**
 * Creates a new function by chaining multiple async functions from left to right.
 *
 * @param  {...any} fns - Functions to chain
 * @returns {function}
 */
const pipe = (...fns) => arg => fns.reduce((d, fn) => d.then(fn), Promise.resolve(arg));


/**
 * Represents a chat message.
 *
 * @typedef {Object} Message
 * @property {'system'|'user'|'assistant'} role
 * @property {string} content
 */

/**
 * A callback function to stream then completion.
 *
 * @callback CompletionHandler
 * @param {string} text
 * @returns {void}
 */

/**
 * Generates a chat completion using a RESTful LLM API service.
 *
 * @param {Array<Message>} messages - List of chat messages.
 * @param {CompletionHandler=} handler - An optional callback to stream the completion.
 * @returns {Promise<string>} The completion generated by the LLM.
 */

const chat = async (messages, handler) => {
    const url = `${LLM_API_BASE_URL}/chat/completions`;
    const auth = LLM_API_KEY ? { 'Authorization': `Bearer ${LLM_API_KEY}` } : {};
    const model = LLM_CHAT_MODEL || 'gpt-3.5-turbo';
    const stop = ['<|im_end|>', '<|end|>', '<|eot_id|>', '<|end_of_turn|>', 'INQUIRY: '];;
    const max_tokens = 400;
    const temperature = 0;
    const stream = LLM_STREAMING && typeof handler === 'function';
    const response = await fetch(url, {
        method: 'POST',
        headers: { 'Content-Type': 'application/json', ...auth },
        body: JSON.stringify({ messages, model, stop, max_tokens, temperature, stream })
    });
    if (!response.ok) {
        throw new Error(`HTTP error with the status: ${response.status} ${response.statusText}`);
    }

    LLM_DEBUG_CHAT && messages.forEach(({ role, content }) => {
        console.log(`${MAGENTA}${role}:${NORMAL} ${content}`);
    });

    if (!stream) {
        const data = await response.json();
        const { choices } = data;
        const first = choices[0];
        const { message } = first;
        const { content } = message;
        const answer = content.trim();
        handler && handler(answer);
        LLM_DEBUG_CHAT && console.log(`${YELLOW}${answer}${NORMAL}`);
        return answer;
    }

    const parse = (line) => {
        let partial = null;
        const prefix = line.substring(0, 6);
        if (prefix === 'data: ') {
            const payload = line.substring(6);
            try {
                const { choices } = JSON.parse(payload);
                const [choice] = choices;
                const { delta } = choice;
                partial = delta?.content;
            } catch (e) {
                // ignore
            } finally {
                return partial;
            }
        }
        return partial;
    }

    const reader = response.body.getReader();
    const decoder = new TextDecoder();

    let answer = '';
    let buffer = '';
    while (true) {
        const { value, done } = await reader.read();
        if (done) {
            break;
        }
        const lines = decoder.decode(value).split('\n');
        for (let i = 0; i < lines.length; ++i) {
            const line = buffer + lines[i];
            if (line[0] === ':') {
                buffer = '';
                continue;
            }
            if (line === 'data: [DONE]') {
                break;
            }
            if (line.length > 0) {
                const partial = parse(line);
                if (partial === null) {
                    buffer = line;
                } else if (partial && partial.length > 0) {
                    buffer = '';
                    if (answer.length < 1) {
                        const leading = partial.trim();
                        answer = leading;
                        handler && (leading.length > 0) && handler(leading);
                    } else {
                        answer += partial;
                        handler && handler(partial);
                    }
                }
            }
        }
    }
    return answer;
}

const PREDEFINED_KEYS = ['INQUIRY', 'TOOL', 'THOUGHT', 'KEYPHRASES', 'OBSERVATION', 'TOPIC'];

/**
 * Break downs a multi-line text based on a number of predefined keys.
 *
 * @param {string} text
 * @returns {Array<string>}
 */

const deconstruct = (text, markers = PREDEFINED_KEYS) => {
    const parts = {};
    const keys = [...markers].reverse();
    const anchor = markers.slice().pop();
    const start = text.lastIndexOf(anchor + ':');
    if (start >= 0) {
        parts[anchor.toLowerCase()] = text.substring(start).replace(anchor + ':', '').trim();
        let str = text.substring(0, start);
        for (let i = 0; i < keys.length; ++i) {
            const marker = keys[i];
            const pos = str.lastIndexOf(marker + ':');
            if (pos >= 0) {
                const substr = str.substr(pos + marker.length + 1).trim();
                const value = substr.split('\n').shift();
                str = str.slice(0, pos);
                const key = marker.toLowerCase();
                parts[key] = value;
            }
        }
    }
    return parts;
}

/**
 * Constructs a multi-line text based on a number of key-value pairs.
 *
 * @param {Object} key-value pairs
 * @return {text}
 */
const construct = (kv) => {
    return PREDEFINED_KEYS.filter(key => kv[key.toLowerCase()]).map(key => {
        const value = kv[key.toLowerCase()];
        if (value && value.length > 0) {
            return `${key.toUpperCase()}: ${value}`;
        }
        return null;
    }).join('\n');
}

/**
 * Represents the record of an atomic processing.
 *
 * @typedef {Object} Stage
 * @property {string} name
 * @property {number} timestamp (Unix epoch)
 * @property {number} duration (in ms)
 */

/**
 * Represents the contextual information for each pipeline stage.
 *
 * @typedef {Object} Context
 * @property {Array<object>} history
 * @property {string} inquiry
 * @property {string} thought
 * @property {string} keyphrases
 * @property {string} observation
 * @property {string} answer
 * @property {Object.<string, function>} delegates - Impure functions to access the outside world.
 */

/**
 * Performs a basic step-by-step reasoning, in the style of Chain of Thought.
 * The updated context will contains new information such as `keyphrases` and `observation`.
 *
 * @param {Context} context - Current pipeline context.
 * @returns {Context} Updated pipeline context.
 */

const REASON_PROMPT = `Use Google to search for the answer.
Think step by step. If necessary, refer to the relevant part of the previous conversation history.

Always output your thought in the following format:

TOOL: the search engine to use (must be Google).
THOUGHT: describe your thoughts about the inquiry.
KEYPHRASES:  the important query to give to Google.
OBSERVATION: the concise result of the search tool.
TOPIC: the specific topic covering the inquiry.`;

const REASON_EXAMPLE = `

# Example

Given an inquiry "What is Pitch Lake in Trinidad famous for?", you will output:

TOOL: Google.
THOUGHT: This is about geography, I will use Google search.
KEYPHRASES: Pitch Lake in Trinidad fame.
OBSERVATION: Pitch Lake in Trinidad is the largest natural deposit of asphalt.
TOPIC: geography.`;

const breakdown = (hint, completion) => {
    const text = hint + completion;
    let result = deconstruct(text);
    const { topic } = result;
    if (!topic || topic.length === 0) {
        result = deconstruct(text + '\n' + 'TOPIC: general knowledge.');
    }
    return result;
}

const reason = async (context) => {
    const { history, delegates } = context;
    const { enter, leave } = delegates;
    enter && enter('Reason');

    const relevant = history.slice(-3);
    let prompt = REASON_PROMPT;
    if (relevant.length === 0) {
        prompt += REASON_EXAMPLE;
    }

    const messages = [];
    messages.push({ role: 'system', content: prompt });
    relevant.forEach(msg => {
        const { inquiry, topic, thought, keyphrases, answer } = msg;
        const observation = answer;
        messages.push({ role: 'user', content: inquiry });
        const assistant = construct({ tool: 'Google.', thought, keyphrases, observation, topic });
        messages.push({ role: 'assistant', content: assistant });
    });

    const { inquiry } = context;
    messages.push({ role: 'user', content: inquiry });
    const hint = ['TOOL: Google.', 'THOUGHT: '].join('\n');
    messages.push({ role: 'assistant', content: hint });
    const completion = await chat(messages);
    let result = breakdown(hint, completion);
    if (!result.keyphrases || result.keyphrases.length === 0) {
        LLM_DEBUG_CHAT && console.log(`-->${RED}Invalid keyphrases. Trying again...`);
        const hint = ['TOOL: Google.', 'THOUGHT: ' + result.thought, 'KEYPHRASES: '].join('\n');
        messages.pop();
        messages.push({ role: 'assistant', content: hint });
        const completion = await chat(messages);
        result = breakdown(hint, completion);
    }
    const { topic, thought, keyphrases, observation } = result;
    leave && leave('Reason', { topic, thought, keyphrases, observation });
    return { topic, thought, keyphrases, observation, ...context };
}

/**
 * Uses the online search engine to collect relevant information based on the keyphrases.
 * The TOP_K most relevant results will be stored in `references`.
 *
 * @param {Context} context - Current pipeline context.
 * @returns {Context} Updated pipeline context.
 */
const search = async (context) => {
    const { delegates, keyphrases, observation } = context;
    const { enter, leave } = delegates;
    enter && enter('Search');

    const response = await fetch('https://api.tavily.com/search', {
        method: 'POST',
        headers: {
            'Content-Type': 'application/json'
        },
        body: JSON.stringify({
            api_key: TAVILY_API_KEY,
            query: keyphrases,
            max_results: TOP_K,
            include_answer: true
        })
    });
    const data = await response.json();
    if (!response.ok) {
        throw new Error(`Tavily call failed with status: ${response.status}`);
    }
    LLM_DEBUG_SEARCH && console.log('Search result: ', { keyphrases, data });
    const { answer, results = [] } = data;
    let references = [];
    if (Array.isArray(results) && results.length > 0) {
        references = results.slice(0, TOP_K).map((result, i) => {
            const { title, url, content } = result;
            const snippet = content;
            return { position: i + 1, title, url, snippet };
        });
    }

    leave && leave('Search', { answer, references });
    return { ...context, observation: answer || observation, references };
}

/**
 * Responds to the user's recent message using an LLM.
 * The response from the LLM is available as `answer` in the updated context.
 *
 * @param {Context} context - Current pipeline context.
 * @returns {Context} Updated pipeline context.
 */

const RESPOND_PROMPT = `You are a world-renowned research assistant.
You are given a user question, and please write clean, concise and accurate answer to the question.
You will be given a set of related references to the question, each starting with a reference number like [citation:x], where x is a number.
Please use only 3 most relevant references, not all of them.
Cite each reference at the end of each sentence.

You are expected to provide an answer that is accurate, correct, and reflect expert knowledge.
Your answer must maintain an unbiased and professional tone.
Ensure that your answer is written in simple English and does not exceed three sentences in length.

Do not give any information that is not related to the question, and do not repeat.
No need to mention "according to the references..." and other internal references.

After every sentence, always cite the reference with the citation numbers, in the format [citation:x].
If a sentence comes from multiple references, please list all applicable citations, like [citation:3][citation:5].

Here are the set of references:

{REFERENCES}

Remember, don't blindly repeat the references verbatim.
Only supply the answer and do not add any additional commentaries, notes, remarks, and list of references.

And here is the user question:`;

const respond = async (context) => {
    const { delegates } = context;
    const { enter, leave, stream } = delegates;
    enter && enter('Respond');

    const { inquiry, references } = context;

    const messages = [];
    if (references && Array.isArray(references) && references.length > 0) {
        const refs = references.map(ref => {
            const { position, title, snippet } = ref;
            return `[citation:${position}] ${title} - ${snippet}`;
        });

        const prompt = RESPOND_PROMPT.replace('{REFERENCES}', refs.join('\n'));
        messages.push({ role: 'system', content: prompt });
        messages.push({ role: 'user', content: inquiry });
        messages.push({ role: 'assistant', content: 'ANSWER: ' });
    } else {
        console.error('No references to cite');
    }
    const answer = await chat(messages, stream);
    leave && leave('Respond', { inquiry });
    return { answer, ...context };
}

/**
 * Prints the pipeline stages, mostly for troubleshooting.
 *
 * @param {Array<Stage>} stages
 */
const review = (stages) => {
    console.log();
    console.log(`${MAGENTA}Pipeline review ${NORMAL}`);
    console.log('---------------');
    stages.map((stage, index) => {
        const { name, duration, timestamp, ...fields } = stage;
        console.log(`${GREEN}${ARROW} Stage #${index + 1} ${YELLOW}${name} ${GRAY}[${duration} ms]${NORMAL}`);
        Object.keys(fields).map(key => {
            const value = fields[key];
            const str = Array.isArray(value) ? JSON.stringify(value, null, 2) : value.toString();
            console.log(`${GRAY}${key}: ${NORMAL}${str}`);
        });
    });
    console.log();
}

/**
 * Collapses every pair of stages (enter and leave) into one stage,
 * and compute its duration instead of invididual timestamps.
 *
 * @param {Array<object} stage
 * @returns {Array<object>}
 */
const simplify = (stages) => {
    const isOdd = (x) => { return (x % 2) !== 0 };
    return stages.map((stage, index) => {
        if (isOdd(index)) {
            const before = stages[index - 1];
            const duration = stage.timestamp - before.timestamp;
            return { ...stage, duration };
        }
        return stage;
    }).filter((_, index) => isOdd(index));
}

/**
 * Converts an expected answer into a suitable regular expression array.
 *
 * @param {string} match
 * @returns {Array<RegExp>}
 */
const regexify = (match) => {
    const filler = (text, index) => {
        let i = index;
        while (i < text.length) {
            if (text[i] === '/') {
                break;
            }
            ++i;
        }
        return i;
    };

    const pattern = (text, index) => {
        let i = index;
        if (text[i] === '/') {
            ++i;
            while (i < text.length) {
                if (text[i] === '/' && text[i - 1] !== '\\') {
                    break;
                }
                ++i;
            }
        }
        return i;
    };

    const regexes = [];
    let pos = 0;
    while (pos < match.length) {
        pos = filler(match, pos);
        const next = pattern(match, pos);
        if (next > pos && next < match.length) {
            const sub = match.substring(pos + 1, next);
            const regex = RegExp(sub, 'gi');
            regexes.push(regex);
            pos = next + 1;
        } else {
            break;
        }
    }

    if (regexes.length === 0) {
        regexes.push(RegExp(match, 'gi'));
    }

    return regexes;
}

/**
 * Returns all possible matches given a list of regular expressions.
 *
 * @param {string} text
 * @param {Array<RegExp>} regexes
 * @returns {Array<Span>}
 */
const match = (text, regexes) => {
    return regexes.map(regex => {
        const match = regex.exec(text);
        if (!match) {
            return null;
        }
        const [first] = match;
        const { index } = match;
        const { length } = first;
        return { index, length };
    }).filter(span => span !== null);
}

/**
 * Formats the input (using ANSI colors) to highlight the spans.
 *
 * @param {string} text
 * @param {Array<Span>} spans
 * @param {string} color
 * @returns {string}
 */

const highlight = (text, spans, color = BOLD + GREEN) => {
    let result = text;
    spans.sort((p, q) => q.index - p.index).forEach((span) => {
        const { index, length } = span;
        const prefix = result.substring(0, index);
        const content = result.substring(index, index + length);
        const suffix = result.substring(index + length);
        result = `${prefix}${color}${content}${NORMAL}${suffix}`;
    });
    return result;
}

/**
 * Evaluates a test file and executes the test cases.
 *
 * @param {string} filename - The path to the test file.
 */
const evaluate = async (filename) => {
    const identity = (x) => x;

    try {
        let history = [];
        let total = 0;
        let failures = 0;

        const handle = async (line) => {
            const parts = (line && line.length > 0) ? line.split(':') : [];
            if (parts.length >= 2) {
                const role = parts[0];
                const content = line.slice(role.length + 1).trim();
                if (role === 'Story') {
                    console.log();
                    console.log('-----------------------------------');
                    console.log(`Story: ${MAGENTA}${BOLD}${content}${NORMAL}`);
                    console.log('-----------------------------------');
                    history = [];
                } else if (role === 'User') {
                    const inquiry = content;
                    const stages = [];
                    const enter = (name) => { stages.push({ name, timestamp: Date.now() }) };
                    const leave = (name, fields) => { stages.push({ name, timestamp: Date.now(), ...fields }) };
                    const delegates = { enter, leave };
                    const context = { inquiry, history, delegates };
                    console.log();
                    process.stdout.write(`  ${inquiry}\r`);
                    const start = Date.now();
                    const pipeline = pipe(reason, search, respond);
                    const result = await pipeline(context);
                    const duration = Date.now() - start;
                    const { topic, thought, keyphrases, references, answer } = result;
                    history.push({ inquiry, thought, keyphrases, topic, references, answer, duration, stages });
                    ++total;
                } else if (role === 'Assistant') {
                    const expected = content;
                    const last = history.slice(-1).pop();
                    if (!last) {
                        console.error('There is no answer yet!');
                        process.exit(-1);
                    } else {
                        const { inquiry, answer, duration, references, stages } = last;
                        const target = answer;
                        const regexes = regexify(expected);
                        const matches = match(target, regexes);
                        if (matches.length === regexes.length) {
                            console.log(`${GREEN}${CHECK} ${CYAN}${inquiry} ${GRAY}[${duration} ms]${NORMAL}`);
                            console.log(' ', highlight(target, matches));
                            if (references && Array.isArray(references) && references.length > 0) {
                                references.forEach((reference) => {
                                    const { position, url } = reference;
                                    console.log(`  ${GRAY}[${position}] ${url}${NORMAL}`);
                                })
                            }
                            LLM_DEBUG_PIPELINE && review(simplify(stages));
                        } else {
                            ++failures;
                            console.error(`${RED}${CROSS} ${YELLOW}${inquiry} ${GRAY}[${duration} ms]${NORMAL}`);
                            console.error(`Expected ${role} to contain: ${CYAN}${regexes.join(',')}${NORMAL}`);
                            console.error(`Actual ${role}: ${MAGENTA}${target}${NORMAL}`);
                            review(simplify(stages));
                            LLM_DEBUG_FAIL_EXIT && process.exit(-1);
                        }
                    }
                } else if (role === 'Pipeline.Reason.Keyphrases' || role === 'Pipeline.Reason.Topic') {
                    const expected = content;
                    const last = history.slice(-1).pop();
                    if (!last) {
                        console.error('There is no answer yet!');
                        process.exit(-1);
                    } else {
                        const { keyphrases, topic, stages } = last;
                        const target = (role === 'Pipeline.Reason.Keyphrases') ? keyphrases : topic;
                        const regexes = regexify(expected);
                        const matches = match(target, regexes);
                        if (matches.length === regexes.length) {
                            console.log(`    ${ARROW} ${GRAY}${role}:`, highlight(target, matches, GREEN));
                        } else {
                            ++failures;
                            console.error(`${RED}Expected ${role} to contain: ${CYAN}${regexes.join(',')}${NORMAL}`);
                            console.error(`${RED}Actual ${role}: ${MAGENTA}${target}${NORMAL}`);
                            review(simplify(stages));
                            LLM_DEBUG_FAIL_EXIT && process.exit(-1);
                        }
                    }
                } else {
                    console.error(`Unknown role: ${role}!`);
                    handle.exit(-1);
                }
            }
        };

        const trim = (input) => {
            const text = input.trim();
            const marker = text.indexOf('#');
            if (marker >= 0) {
                return text.substr(0, marker).trim();
            }
            return text;
        }

        const lines = fs.readFileSync(filename, 'utf-8').split('\n').map(trim);
        for (const i in lines) {
            await handle(lines[i]);
        }
        if (failures <= 0) {
            console.log(`${GREEN}${CHECK}${NORMAL} SUCCESS: ${GREEN}${total} test(s)${NORMAL}.`);
        } else {
            console.log(`${RED}${CROSS}${NORMAL} FAIL: ${GRAY}${total} test(s), ${RED}${failures} failure(s)${NORMAL}.`);
            process.exit(-1);
        }
    } catch (e) {
        console.error('ERROR:', e.toString());
        process.exit(-1);
    }
}

const interact = async () => {
    const history = [];
    const stream = (text) => process.stdout.write(text);

    let loop = true;
    const io = readline.createInterface({ input: process.stdin, output: process.stdout });
    io.on('close', () => { loop = false; });
    console.log();

    const qa = () => {
        io.question(`${YELLOW}>> ${CYAN}`, async (inquiry) => {
            process.stdout.write(NORMAL);
            if (inquiry === '!review' || inquiry === '/review') {
                const last = history.slice(-1).pop();
                if (!last) {
                    console.log('Nothing to review yet!');
                    console.log();
                } else {
                    const { stages } = last;
                    review(simplify(stages));
                }

            } else {
                const stages = [];
                const update = (stage, fields) => {
                    if (stage === 'Reason') {
                        const { keyphrases } = fields;
                        if (keyphrases && keyphrases.length > 0) {
                            console.log(`${GRAY}${ARROW} Searching for ${keyphrases}...${NORMAL}`);
                        }
                    }
                }
                const enter = (name) => { stages.push({ name, timestamp: Date.now() }) };
                const leave = (name, fields) => { update(name, fields); stages.push({ name, timestamp: Date.now(), ...fields }) };
                const delegates = { stream, enter, leave };
                const context = { inquiry, history, delegates };
                const start = Date.now();
                const pipeline = pipe(reason, search, respond);
                const result = await pipeline(context);
                const { topic, thought, keyphrases } = result;
                const duration = Date.now() - start;
                const { answer, references } = result;
                if (references && Array.isArray(references) && references.length > 0) {
                    console.log();
                    console.log();
                    references.forEach((reference) => {
                        const { position, url } = reference;
                        console.log(`[${position}] ${GRAY}${url}${NORMAL}`);
                    })
                }
                history.push({ inquiry, thought, keyphrases, topic, answer, duration, stages });
                console.log();
                console.log();
            }
            loop && qa();
        })
    }

    qa();
}

(async () => {
    if (!TAVILY_API_KEY || TAVILY_API_KEY.length < 32) {
        console.error('Fatal error: TAVILY_API_KEY not set!');
        process.exit(-1);
    }
    console.log(`Using LLM at ${LLM_API_BASE_URL} (model: ${GREEN}${LLM_CHAT_MODEL || 'default'}${NORMAL}).`);

    const args = process.argv.slice(2);
    args.forEach(evaluate);
    if (args.length == 0) {
        await interact();
    }
})();
